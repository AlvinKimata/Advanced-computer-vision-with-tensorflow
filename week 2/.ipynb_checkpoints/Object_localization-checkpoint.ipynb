{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 16:42:47.845811: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-27 16:42:47.845843: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "from utils import (draw_bounding_box_on_image,\n",
    "                    draw_bounding_boxes_on_image,\n",
    "                    draw_bounding_boxes_on_image_array)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print('Tensorflow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions used for visualization.\n",
    "plt.rc('image', cmap = 'gray')\n",
    "plt.rc('grid', linewidth = 0)\n",
    "plt.rc('xtick', top = False, bottom = False, labelsize = 'large')\n",
    "plt.rc('ytick', left = False, right = False, labelsize = 'large')\n",
    "plt.rc('axes', facecolor = 'F8F8F8', titlesize = 'large', edgecolor = 'white')\n",
    "plt.rc('text', color = 'a8151a')\n",
    "plt.rc('figure', facecolor = 'F0F0F0')\n",
    "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), 'mpl-data/fonts/ttf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull a batch from the datasets.\n",
    "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\n",
    "    \n",
    "    #Get one batch from each: 10_000 validation digits, N training digits.\n",
    "    batch_train_ds = training_dataset.unbatch().batch(N)\n",
    "    \n",
    "    #Eager execution: loop through the datasets normally.\n",
    "    if tf.executing_eagerly():\n",
    "        for validation_digits, (validation_labels, validation_bboxes) in validation_dataset:\n",
    "            validation_digits = validation_digits.numpy()\n",
    "            validation_labels = validation_labels.numpy()\n",
    "            validation_bboxes = validation_bboxes.numpy()\n",
    "            break\n",
    "            \n",
    "        for training_digits, (training_labels, training_bboxes) in batch_train_ds:\n",
    "            training_digits = training_digits.numpy()\n",
    "            training_labels = training_labels.numpy()\n",
    "            training_bboxes = training_bboxes.numpy()\n",
    "            break\n",
    "    \n",
    "    #These were one-hot encoded in the dataset.\n",
    "    validation_labels = np.argmax(validation_labels, axis = 1)\n",
    "    training_labels = np.argmax(training_labels, axis = 1)\n",
    "    \n",
    "    return (training_digits, training_labels, training_bboxes,\n",
    "           validation_digits, validation_labels, validation_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create digits from local fonts for testing.\n",
    "def create_digits_from_local_fonts(n):\n",
    "    font_labels = []\n",
    "    #format 'LA': black in channel 0, alpha in channel 1\n",
    "    img = PIL.Image.new('LA', (75 * n, 75), color = (0, 255)) \n",
    "    font1 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'DejaVuSansMono-Oblique.ttf'), 25)\n",
    "    font2 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'STIXGeneral.ttf'), 25)\n",
    "    d = PIL.ImageDraw.Draw(img)\n",
    "    \n",
    "    for item in range(n):\n",
    "        font_labels.append(item % 10)\n",
    "        d.text((7 + item * 75, 0 if 1 < 10 else -4), str(item % 10), fill = (255, 255), font = font1 if 1 < 10 else font2)\n",
    "    \n",
    "    font_digits = np.array(img.getdata(), np.float32)[:, 0] / 255.0 \n",
    "    font_digits = np.reshape(np.stack(np.split(np.reshape(font_digits, [75, 75 * n]), n, axis = 1), axis = 0), [n, 75 * 75])\n",
    "    return font_digits, font_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to display a row of digits with their predictions.\n",
    "def display_digits_with_boxes(digits, predictions, labels, pred_bboxes, bboxes, iou, title):\n",
    "    n = 10\n",
    "    \n",
    "    indices = np.random.choice(len(predictions), size = n)\n",
    "    n_digits = digits[indices]\n",
    "    n_predictions = predictions[indices]\n",
    "    n_labels = labels[indices]\n",
    "    \n",
    "    n_iou = []\n",
    "    if len(iou) > 0:\n",
    "        n_iou = iou[indices]\n",
    "        \n",
    "    if (len(pred_bboxes) > 0):\n",
    "        n_pred_bboxes = pred_bboxes[indices, :]\n",
    "    \n",
    "    if (len(bboxes) > 0):\n",
    "        n_bboxes = bboxes[indices,:]\n",
    "    \n",
    "    n_digits = n_digits * 255.0\n",
    "    n_digits = n_digits.reshape(n, 75, 75)\n",
    "    fig = plt.figure(figsize = (20, 4))\n",
    "    plt.title(title)\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    \n",
    "    for item in range(10):\n",
    "        ax = fig.add_subplot(1, 10, item + 1)\n",
    "        bboxes_to_plot = []\n",
    "        if (len(pred_bboxes) > item):\n",
    "            bboxes_to_plot.append(n_pred_bboxes[item])\n",
    "            \n",
    "        if (len(bboxes) > item):\n",
    "            bboxes_to_plot.append(n_bboxes[item])\n",
    "            \n",
    "        img_to_draw = draw_bounding_boxes_on_image_array(image = n_digits[item],\n",
    "                                                         boxes = np.asarray(bboxes_to_plot), color = ['red', 'green'],\n",
    "                                                        display_str_list = ['true', 'pred'])\n",
    "        plt.xlabel(n_predictions[item])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "        if n_predictions[item] != n_labels[item]:\n",
    "            ax.xaxis.label.set_color('red')\n",
    "            \n",
    "        plt.imshow(img_to_draw)\n",
    "        \n",
    "        if len(iou) > item:\n",
    "            color = 'black'\n",
    "            if (n_iou[item][0] < iou_treshold):\n",
    "                color = 'red'\n",
    "            ax.text(0.2, -0.3, f'iou: {n_iou[item][0]}', color = color, \n",
    "                   transform = ax.transAxes)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to display training and validation curves.\n",
    "def plot_metrics(metric_name, title, ylim = 5):\n",
    "    plt.title(title)\n",
    "    plt.ylim(0, ylim)\n",
    "    plt.plot(history.history[metric_name], color = 'blue', label = metric_name)\n",
    "    plt.plot(history.history['val_' + metric_name], color = 'green', label = 'val_' + metric_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select between strategies.\n",
    "***\n",
    "\n",
    "Depending on the hardware available, you'll use different distribution strategies.\n",
    "- If a TPU is available, then you'll be using the TPU strategy. \n",
    "- If more than one GPU is available, then you'll use the Mirrored Strategy.\n",
    "- If one GPU is available or the CPU is available, you'll use the default strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "Number of accelerators:  1\n"
     ]
    }
   ],
   "source": [
    "#Detect hardware.\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    \n",
    "except ValueError:\n",
    "    tpu = None\n",
    "    gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    \n",
    "#Select appropriate distribution strategies.\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu) \n",
    "    print(f'Running on TPU {tpu.cluster_spec().as_dict()[\"worker\"]}')\n",
    "\n",
    "elif len(gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
    "    print(f'Running on multiple GPUs', [gpu.name for gpu in gpus])\n",
    "    \n",
    "elif len(gpus) == 1:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(f'Running on GPU{gpus[0].name}')\n",
    "    \n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Running on CPU.')\n",
    "\n",
    "print('Number of accelerators: ', strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters.\n",
    "The global batch size is the batch size per replica(64 in this case) times the number of replicas in the distribution strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the dataset.\n",
    "***\n",
    "Define some helper functions that will pre-process your data:\n",
    "- `read_image_tfds`: randomly overlays the \"digit\" image on top of a larger canvas.\n",
    "- `get_training_dataset`: loads data and splits it to get the training set.\n",
    "- `get_validation_dataset`: loads and splits the data to get the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_tfds(image, label):\n",
    "    '''\n",
    "    Transforms each image in the dataset by pasting it on\n",
    "    a 75 * 75 canvas at random locations.\n",
    "    '''\n",
    "    xmin = tf.random.uniform((), 0, 48, dtype = tf.int32)\n",
    "    ymin = tf.random.uniform((), 0, 48, dtype = tf.int32)\n",
    "    image = tf.reshape(image, (28, 28, 1))\n",
    "    image = tf.image.pad_to_bounding_box(image, ymin, xmin, 75, 75)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    xmin = tf.cast(xmin, tf.float32)\n",
    "    ymin = tf.cast(ymin, tf.float32)\n",
    "    \n",
    "    xmax = (xmin + 28) / 75\n",
    "    ymax = (ymin + 28) / 75\n",
    "    xmin = xmin / 75\n",
    "    ymin = ymin / 75\n",
    "    return image, (tf.one_hot(label, 10), [xmin, ymin, xmax, ymax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset():\n",
    "    '''\n",
    "    Loads and maps the training split of the dataset\n",
    "    using the map function. Note that we try to load the GCS \n",
    "    version since TPU can only work with datasets on GCS.\n",
    "    '''\n",
    "    with strategy.scope():\n",
    "        dataset = tfds.load('mnist', split = 'train', as_supervised = True, try_gcs = True)\n",
    "        dataset = (dataset\n",
    "            .map(read_image_tfds, num_parallel_calls = 16)\n",
    "            .shuffle(5_000, reshuffle_each_iteration = True)\n",
    "            .repeat()\n",
    "            .batch(BATCH_SIZE, drop_remainder = True)\n",
    "            .prefetch(-1)\n",
    "        )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_validation_dataset():\n",
    "    '''\n",
    "    Loads and maps the validation split of the dataset using \n",
    "    the `map` function.\n",
    "    '''\n",
    "    dataset = tfds.load(\"mnist\", split=\"test\", as_supervised=True, try_gcs=True)\n",
    "    dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
    "\n",
    "    dataset = dataset.cache() # this small dataset can be entirely cached in RAM\n",
    "    dataset = dataset.batch(10000, drop_remainder=True) # 10000 items in eval dataset, all in one batch\n",
    "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
    "    return dataset\n",
    "\n",
    "#Instantiate datasets.\n",
    "with strategy.scope():\n",
    "    training_dataset = get_training_dataset()\n",
    "    validation_dataset = get_validation_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 16:51:34.623417: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5600 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_195949/2197895922.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                           training_bboxes, np.array([]), 'training digits and their labels')\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m display_digits_with_boxes(validation_digits, validation_labels, validation_labels, np.array([]),\n\u001b[0m\u001b[1;32m      9\u001b[0m                           training_bboxes, np.array([]), 'validation digits and their labels')\n",
      "\u001b[0;32m/tmp/ipykernel_195949/1934315704.py\u001b[0m in \u001b[0;36mdisplay_digits_with_boxes\u001b[0;34m(digits, predictions, labels, pred_bboxes, bboxes, iou, title)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mn_bboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mn_digits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_digits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5600 is out of bounds for axis 0 with size 10"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 11 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(training_digits, training_labels, training_bboxes, \n",
    "validation_digits, validation_labels, validation_bboxes) = dataset_to_numpy_util(training_dataset, validation_dataset, 10)\n",
    "\n",
    "\n",
    "display_digits_with_boxes(training_digits, training_labels, training_labels, np.array([]),\n",
    "                          training_bboxes, np.array([]), 'training digits and their labels')\n",
    "\n",
    "display_digits_with_boxes(validation_digits, validation_labels, validation_labels, np.array([]),\n",
    "                          training_bboxes, np.array([]), 'validation digits and their labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network.\n",
    "Here, you'll define your own custom CNN.\n",
    "- `feature_extractor`: These covolutional layers extract the features of the image.\n",
    "- `classifier`: This defines the output layer that predicts among 10 categories.\n",
    "- `bounding_box_regression`: This defines the output layer that predicts 4 numeric values which define the coordinates of the bounding box [xmin, ymin, xmax, ymax]\n",
    "\n",
    "- `final_model`: This combines the layers for feature extraction, classification and bounding box prediction.\n",
    "\n",
    "    - Notice that this is another example of a branching model, because the model splits to produce two kinds of output (a category and set of numbers).  \n",
    "    \n",
    "- `define_and_compile_model`: choose the optimizer and metrics, then compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(inputs):\n",
    "    x = tf.keras.layers.Conv2D(16, activation = 'relu', kernel_size = 3, input_shape = (75, 75, 1))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size = 3, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size = 3, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layers(inputs):\n",
    "    x = tf.keras.layers.Flatten()(inputs)\n",
    "    x = tf.keras.layers.Dense(128, activation = 'relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(inputs):\n",
    "    classification_output = tf.keras.layers.Dense(10, activation = 'softmax', name = 'classification')(inputs)\n",
    "    return classification_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box_regression(inputs):\n",
    "    '''\n",
    "    This function defines the regression output from bounding box predictions.\n",
    "    Note that we have 4 outputs corresponding to [xmin, ymin, xmax, ymax]\n",
    "    '''\n",
    "    bounding_box_regression_output = tf.keras.layers.Dense(units = 4, name = 'bounding_box')(inputs)\n",
    "    return bounding_box_regression_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(inputs):\n",
    "    feature_cnn = feature_extractor(inputs)\n",
    "    dense_output = dense_layers(feature_cnn)\n",
    "    \n",
    "    '''\n",
    "    The model branches here.\n",
    "    The dense layer's output gets fed into 2 branches:\n",
    "    classification_output and bounding_box_output.\n",
    "    '''\n",
    "    classification_output = classifier(dense_output)\n",
    "    bounding_box_output = bounding_box_regression(dense_output)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = inputs, outputs = [classification_output, bounding_box_output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 75, 75, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 73, 73, 16)   160         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " average_pooling2d_9 (AveragePo  (None, 36, 36, 16)  0           ['conv2d_9[0][0]']               \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 34, 34, 32)   4640        ['average_pooling2d_9[0][0]']    \n",
      "                                                                                                  \n",
      " average_pooling2d_10 (AverageP  (None, 17, 17, 32)  0           ['conv2d_10[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 15, 15, 64)   18496       ['average_pooling2d_10[0][0]']   \n",
      "                                                                                                  \n",
      " average_pooling2d_11 (AverageP  (None, 7, 7, 64)    0           ['conv2d_11[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 3136)         0           ['average_pooling2d_11[0][0]']   \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          401536      ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " classification (Dense)         (None, 10)           1290        ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " bounding_box (Dense)           (None, 4)            516         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 426,638\n",
      "Trainable params: 426,638\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def define_and_compile_model(inputs):\n",
    "    model = final_model(inputs)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', \n",
    "                 loss = {'classification': 'categorical_crossentropy',\n",
    "                        'bounding_box': 'mse'\n",
    "                        },\n",
    "                 metrics = {'classification': 'accuracy',\n",
    "                          'bounding_box': 'mse'})\n",
    "    \n",
    "    return model\n",
    "\n",
    "with strategy.scope():\n",
    "    inputs = tf.keras.Input(shape = (75, 75, 1))\n",
    "    model = define_and_compile_model(inputs)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model.  \n",
    "- You can choose the number of epochs depending on the level of performance that you want and the time that you have.\n",
    "- Each epoch will take just a few seconds if you're using the TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.9813 - classification_loss: 0.9642 - bounding_box_loss: 0.0171 - classification_accuracy: 0.6651 - bounding_box_mse: 0.0171"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 17:29:13.199679: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3410560000 exceeds 10% of free system memory.\n",
      "2022-07-27 17:29:16.384977: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 829440000 exceeds 10% of free system memory.\n",
      "2022-07-27 17:29:18.606054: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1479680000 exceeds 10% of free system memory.\n",
      "2022-07-27 17:29:21.665930: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 576000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937/937 [==============================] - 121s 122ms/step - loss: 0.9813 - classification_loss: 0.9642 - bounding_box_loss: 0.0171 - classification_accuracy: 0.6651 - bounding_box_mse: 0.0171 - val_loss: 0.2722 - val_classification_loss: 0.2635 - val_bounding_box_loss: 0.0087 - val_classification_accuracy: 0.9238 - val_bounding_box_mse: 0.0087\n",
      "Epoch 2/10\n",
      "488/937 [==============>...............] - ETA: 50s - loss: 0.2823 - classification_loss: 0.2749 - bounding_box_loss: 0.0074 - classification_accuracy: 0.9171 - bounding_box_mse: 0.0074"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_195949/991440571.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m history = model.fit(training_dataset,\n\u001b[0m\u001b[1;32m      6\u001b[0m                     steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10 # 45\n",
    "steps_per_epoch = 60000//BATCH_SIZE  # 60,000 items in this dataset\n",
    "validation_steps = 1\n",
    "\n",
    "history = model.fit(training_dataset,\n",
    "                    steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)\n",
    "\n",
    "loss, classification_loss, bounding_box_loss, classification_accuracy, bounding_box_mse = model.evaluate(validation_dataset, steps=1)\n",
    "print(\"Validation accuracy: \", classification_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_195949/2270745513.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'classification_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Classification_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bounding_box_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Bounding Box Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_195949/3226216853.py\u001b[0m in \u001b[0;36mplot_metrics\u001b[0;34m(metric_name, title, ylim)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'green'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics('classification_loss', 'Classification_loss')\n",
    "plot_metrics('bounding_box_loss', 'Bounding Box Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection over Union.\n",
    "Calculate the IOU metric to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(pred_box, true_box):\n",
    "    xmin_pred, ymin_pred, xmax_pred, ymax_pred = np.split(pred_box, 4, axis = 1)\n",
    "    xmin_true, ymin_true, xmax_true, ymax_true = np.split(true_box, axis = 1)\n",
    "    \n",
    "    smoothing_factor = 1e-10\n",
    "    \n",
    "    xmin_overlap = np.maximum(xmin_pred, xmin_true)\n",
    "    xmax_overlap = np.minimum(xmax_pred, xmax_true)\n",
    "    \n",
    "    ymin_overlap = np.maximum(ymin_pred, ymin_true)\n",
    "    ymax_overlap = np.minimum(ymax_pred, ymax_true)\n",
    "    \n",
    "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
    "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
    "    \n",
    "    overlap_area = np.maximum((xmax_overlap - xmin_overlap), 0) * np.maximum((ymax_overlap - ymin_overlap), 0)\n",
    "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
    "    \n",
    "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 37ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_split_dispatcher() missing 1 required positional argument: 'indices_or_sections'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_195949/3910529196.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredicted_bboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintersection_over_union\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_bboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0miou_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_195949/3463780674.py\u001b[0m in \u001b[0;36mintersection_over_union\u001b[0;34m(pred_box, true_box)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mintersection_over_union\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mxmin_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mxmin_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msmoothing_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _split_dispatcher() missing 1 required positional argument: 'indices_or_sections'"
     ]
    }
   ],
   "source": [
    "##Recognize validation digits\n",
    "predictions = model.predict(validation_digits, batch_size=64)\n",
    "predicted_labels = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "predicted_bboxes = predictions[1]\n",
    "\n",
    "iou = intersection_over_union(predicted_bboxes, validation_bboxes)\n",
    "\n",
    "iou_threshold = 0.6\n",
    "\n",
    "print(\"Number of predictions where iou > threshold(%s): %s\" % (iou_threshold, (iou >= iou_threshold).sum()))\n",
    "print(\"Number of predictions where iou < threshold(%s): %s\" % (iou_threshold, (iou < iou_threshold).sum()))\n",
    "\n",
    "\n",
    "display_digits_with_boxes(validation_digits, predicted_labels, validation_labels, predicted_bboxes, validation_bboxes, iou, \"True and Predicted values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0570141520b63c28470ecad6ff8542a49d29754cf9906cbc70e42fc736766dbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
