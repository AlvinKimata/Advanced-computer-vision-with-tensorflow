{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540a473c-084c-4d09-aed0-b2d010bfa919",
   "metadata": {},
   "source": [
    "## This notebook illustrates fine tuning a `RetinaNet` object detection model for object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd86aea-dc34-41d9-ac3e-5290fec1a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be127999-1d7f-496a-aa4d-f0d791677948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-05 23:12:11.783621: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-05 23:12:12.407336: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-05 23:12:12.407413: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-05 23:12:12.614280: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-05 23:12:16.854777: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-05 23:12:16.855159: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-05 23:12:16.855211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/debonair/anaconda3/lib/python3.9/site-packages/google/colab/data_table.py:30: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils import traitlets as _traitlets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import glob\n",
    "import random\n",
    "import imageio\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util, config_util, colab_utils\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5aad07-e1eb-4628-a394-21cedcee3df9",
   "metadata": {},
   "source": [
    "## Utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29add728-6ba4-4458-807f-2b2d602903e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    '''Load an image from a file into a numpy array.\n",
    "    \n",
    "    Args:\n",
    "        path: A file path.\n",
    "    Returns:\n",
    "        uint8 numpy array with shape (im_height, im_width, 3)\n",
    "    '''\n",
    "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "    image = Image.open(BytesIO(img_data))\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        im_height, im_width, 3).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44571ea8-9755-4457-89e5-b3265eaf9df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detections(image_np, boxes, classes, \n",
    "                    scores, category_index, figsize = (12, 16), \n",
    "                    image_name = None):\n",
    "    '''Wrapper function to visualize detections.\n",
    "    Args:\n",
    "        image_np: uint8 numpy array with shape (im_height, im_width, 3)\n",
    "        boxes: A numpy array of shape [N, 4]\n",
    "        classes: A numpy array of shape [N]\n",
    "        scores: A numpy array of shape [N] or None. If scores = None, this \n",
    "            function assumes that the boxes plotted are groundtruth boxes and plot all boxes as black with no classes or scores.\n",
    "            \n",
    "        category_index: A dictionary containing category index and category names.\n",
    "        figsize: Size of the figure.\n",
    "        image_name: A name for the image file.\n",
    "    '''\n",
    "    image_np = np.array(image_np)\n",
    "    image_np_with_annotations = image_np.copy()\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_annotations, boxes, \n",
    "        classes, scores, category_index, \n",
    "        use_normalized_coordinates=True, \n",
    "        min_score_thresh=0.8)\n",
    "    \n",
    "    if image_name:\n",
    "        plt.imsave(image_name, image_np_with_annotations)\n",
    "    else:\n",
    "        plt.imshow(image_np_with_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2974646-112d-4fe7-9c45-db8b0069d06e",
   "metadata": {},
   "source": [
    "## Load dataset.\n",
    "### The dataset used is the `Rubber duck dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a8ba29b-211d-419b-a5ea-65fb6d34b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93277/2906057302.py:22: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "train_image_dir = '../inputs/rubber_duck/train/'\n",
    "train_images_np = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    image_path = os.path.join(train_image_dir, f'robertducky{i}.jpg')\n",
    "    \n",
    "    train_images_np.append(load_image_into_numpy_array(image_path))\n",
    "    \n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['xtick.labelsize'] = False\n",
    "plt.rcParams['ytick.labelsize'] = False\n",
    "plt.rcParams['xtick.top'] = False\n",
    "plt.rcParams['xtick.bottom'] = False\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['ytick.right'] = False\n",
    "plt.rcParams['figure.figsize'] = [14, 7]\n",
    "\n",
    "for idx, train_image_np in enumerate(train_images_np):\n",
    "    plt.subplot(2, 3, idx + 1)\n",
    "    plt.imshow(train_image_np)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afef10e-822c-48ca-9391-4e0bac021f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Annotate image with bounding boxes.\n",
    "\n",
    "gt_boxes = [\n",
    "            np.array([[0.436, 0.591, 0.629, 0.712]], dtype=np.float32),\n",
    "            np.array([[0.539, 0.583, 0.73, 0.71]], dtype=np.float32),\n",
    "            np.array([[0.464, 0.414, 0.626, 0.548]], dtype=np.float32),\n",
    "            np.array([[0.313, 0.308, 0.648, 0.526]], dtype=np.float32),\n",
    "            np.array([[0.256, 0.444, 0.484, 0.629]], dtype=np.float32)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c6288d-cb06-4732-af5a-d64638a1b070",
   "metadata": {},
   "source": [
    "## Prepare data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2154237b-4748-41d3-ab06-540719a2db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-05 23:12:32.436179: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-05 23:12:32.436253: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-05 23:12:32.436301: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (debonair): /proc/driver/nvidia/version does not exist\n",
      "2022-11-05 23:12:32.436889: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "duck_class_id = 1\n",
    "num_classes = 1\n",
    "\n",
    "category_index = {duck_class_id: {'id': duck_class_id, \n",
    "                                  'name': 'rubber ducky'}}\n",
    "\n",
    "#Convert class labels to one-hot.\n",
    "\n",
    "#The label_id_offset shifts all classes by a certain number of indices.\n",
    "#We do this so that the mode receives one-hot labels where non-backgroud \n",
    "#classes start counting at the 0th index.\n",
    "\n",
    "label_id_offset = 1\n",
    "train_image_tensors = []\n",
    "gt_classes_one_hot_tensors = []\n",
    "gt_box_tensors = []\n",
    "\n",
    "for (train_image_np, gt_box_np) in zip(train_images_np, gt_boxes):\n",
    "    train_image_np = tf.expand_dims(tf.convert_to_tensor(train_image_np, dtype = tf.float32), axis = 0)\n",
    "    train_image_tensors.append(train_image_np)\n",
    "    \n",
    "    gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
    "    \n",
    "    zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
    "        np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
    "    \n",
    "    gt_classes_one_hot_tensors.append(tf.one_hot(zero_indexed_groundtruth_classes, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "847296e2-29ee-412d-a1f8-54904a5d050f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93277/2960753525.py:11: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "dummy_scores = np.array([1.0], dtype = np.float32)\n",
    "\n",
    "plt.figure(figsize = (30, 15))\n",
    "for idx in range(5):\n",
    "    plt.subplot(2, 3, idx + 1)\n",
    "    plot_detections(train_images_np[idx], \n",
    "                    gt_boxes[idx], \n",
    "                    np.ones(shape = [gt_boxes[idx].shape[0]], dtype = np.int32),\n",
    "                    dummy_scores, category_index)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd714ab-0e76-4336-acff-438020fc84c7",
   "metadata": {},
   "source": [
    "### Prepare model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "186799e1-a09e-4db2-91f2-bf0992cd6756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-05 22:11:09--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
      "Resolving download.tensorflow.org (download.tensorflow.org)... 216.58.223.112, 2a00:1450:401a:805::2010\n",
      "Connecting to download.tensorflow.org (download.tensorflow.org)|216.58.223.112|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 244817203 (233M) [application/x-tar]\n",
      "Saving to: ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz’\n",
      "\n",
      "ssd_resnet50_v1_fpn 100%[===================>] 233.48M  1.01MB/s    in 3m 28s  \n",
      "\n",
      "2022-11-05 22:14:38 (1.12 MB/s) - ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz’ saved [244817203/244817203]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download model weights.\n",
    "!wget 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz'\n",
    "\n",
    "!mv 'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz' 'models/'\n",
    "\n",
    "#Extract model weights.\n",
    "!tar -xf 'models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1eda5f5a-eb4c-4385-9d84-4f7a7c83d1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-05 22:18:34--  https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4528 (4.4K) [text/plain]\n",
      "Saving to: ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config’\n",
      "\n",
      "ssd_resnet50_v1_fpn 100%[===================>]   4.42K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-11-05 22:18:36 (24.2 MB/s) - ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config’ saved [4528/4528]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download config file.\n",
    "!wget 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "!mv 'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config' 'models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dde312e-16a9-4c58-a3fd-34bce3e041f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and restoring weights for fine-tuning...\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print('Building model and restoring weights for fine-tuning...', flush = True)\n",
    "\n",
    "num_classes = 1\n",
    "pipeline_config = 'models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "checkpoint_path = 'models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0'\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "model_config.ssd.num_classes = num_classes\n",
    "model_config.ssd.freeze_batchnorm = True\n",
    "\n",
    "detection_model = model_builder.build(model_config = model_config, is_training = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c57c6a22-a3e4-4f33-b2eb-5e14849d66ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights restored\n"
     ]
    }
   ],
   "source": [
    "fake_box_predictor = tf.compat.v2.train.Checkpoint(_base_tower_layers_for_heads = detection_model._box_predictor._base_tower_layers_for_heads,\n",
    "                                                   _box_predictor_head = detection_model._box_predictor._box_prediction_head)\n",
    "\n",
    "fake_model = tf.compat.v2.train.Checkpoint(_feature_extractor = detection_model._feature_extractor, \n",
    "                                           _box_predictor = fake_box_predictor)\n",
    "\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model = fake_model)\n",
    "ckpt.restore(checkpoint_path).expect_partial()\n",
    "\n",
    "#Run model through a dummy image so that variables are created.\n",
    "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
    "predictions_dict = detection_model.predict(image, shapes)\n",
    "_ = detection_model.postprocess(predictions_dict, shapes)\n",
    "print('Weights restored')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab201bf9-a273-4958-af00-97e58412a03e",
   "metadata": {},
   "source": [
    "### Eager mode custom training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aad6b3f-40df-45d0-8601-7bcb205c07cf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debonair/anaconda3/lib/python3.9/site-packages/keras/backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.set_learning_phase(True)\n",
    "\n",
    "# These parameters can be tuned; since our training set has 5 images.\n",
    "# It doesn't make sense to have a much larger batch size, though we could\n",
    "# fit more examples in memory if we wanted to.\n",
    "\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "num_batches = 100\n",
    "\n",
    "#Select variables in top layers to fine-tune.\n",
    "trainable_variables = detection_model.trainable_variables\n",
    "to_fine_tune = []\n",
    "prefixes_to_train = [\n",
    "    'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
    "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
    "\n",
    "for var in trainable_variables:\n",
    "    if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
    "        to_fine_tune.append(var)\n",
    "        \n",
    "        \n",
    "#Set up a forward + backward pass for a single train step.\n",
    "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
    "    '''Get a tf.function for training step/'''\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step_fn(image_tensors, \n",
    "                      groundtruth_boxes_list, \n",
    "                      groundtruth_classes_list):\n",
    "        '''A single training iteration\n",
    "        Args:\n",
    "            image_tensors: A list of [1, height, width, 3] tensors of type tf.float32.\n",
    "            groundtruth_boxes_list: A list of tensors of shape [N_i, 4] with type tf.float32 representing groundtruth boxes for each image in the batch.\n",
    "            groundtruth_classes_list: A list of tensors of shape [N_i, num_classes with type tf.float32 representing groundtruth boxes for each image in the batch.\n",
    "        \n",
    "        Returns:\n",
    "            A scalar tensor representing the total loss for the input batch.\n",
    "        '''\n",
    "        \n",
    "        shapes = tf.constant(batch_size * [[640, 640, 3]], dtype = tf.int32)\n",
    "        model.provide_groundtruth(\n",
    "            groundtruth_boxes_list = groundtruth_boxes_list,\n",
    "            groundtruth_classes_list = groundtruth_classes_list)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            preprocessed_images = tf.concat(\n",
    "                [detection_model.preprocess(image_tensor)[0] for image_tensor in image_tensors], axis = 0)\n",
    "            \n",
    "            prediction_dict = model.predict(preprocessed_images, shapes)\n",
    "            losses_dict = model.loss(prediction_dict, shapes)\n",
    "            total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
    "            gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
    "            optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
    "            \n",
    "        return total_loss\n",
    "    return train_step_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b635e516-559c-47fc-8356-d045f7154ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fine-tuning...\n",
      "Batch 0 of 100, loss = 1.7429404258728027\n",
      "Batch 10 of 100, loss = 0.2845860719680786\n",
      "Batch 20 of 100, loss = 0.08820019662380219\n",
      "Batch 30 of 100, loss = 0.047110289335250854\n",
      "Batch 40 of 100, loss = 0.013916511088609695\n",
      "Batch 50 of 100, loss = 0.0064419968985021114\n",
      "Batch 60 of 100, loss = 0.004060050006955862\n",
      "Batch 70 of 100, loss = 0.002770668128505349\n",
      "Batch 80 of 100, loss = 0.001954255159944296\n",
      "Batch 90 of 100, loss = 0.0023290542885661125\n",
      "Done fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate, momentum = 0.9)\n",
    "train_step_fn = get_model_train_step_function(detection_model, optimizer, to_fine_tune)\n",
    "\n",
    "print('Start fine-tuning...', flush = True)\n",
    "for idx in range(num_batches):\n",
    "    #Grab keys for a random subset of examples.\n",
    "    all_keys = list(range(len(train_images_np)))\n",
    "    random.shuffle(all_keys)\n",
    "    example_keys = all_keys[:batch_size]\n",
    "    \n",
    "    gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
    "    gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
    "    image_tensors = [train_image_tensors[key] for key in example_keys]\n",
    "    \n",
    "    #Training step (forward pass + backward pass)\n",
    "    total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        print(f'Batch {idx} of {num_batches}, loss = {total_loss.numpy()}', flush = True)\n",
    "        \n",
    "print('Done fine-tuning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a8517-c150-4f28-9930-39012c257efa",
   "metadata": {},
   "source": [
    "### Load test images to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bf33d50-a7d8-4d18-a1e9-6ffe265cb828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../inputs/rubber_duck/test/out1.jpg\n",
      "../inputs/rubber_duck/test/out2.jpg\n",
      "../inputs/rubber_duck/test/out3.jpg\n",
      "../inputs/rubber_duck/test/out4.jpg\n",
      "../inputs/rubber_duck/test/out5.jpg\n",
      "../inputs/rubber_duck/test/out6.jpg\n",
      "../inputs/rubber_duck/test/out7.jpg\n",
      "../inputs/rubber_duck/test/out8.jpg\n",
      "../inputs/rubber_duck/test/out9.jpg\n"
     ]
    }
   ],
   "source": [
    "test_image_dir = '../inputs/rubber_duck/test/'\n",
    "test_images_np = []\n",
    "\n",
    "for item in range(1,10):\n",
    "    image_path = os.path.join(test_image_dir, 'out' + str(item) + '.jpg')\n",
    "    test_image = np.expand_dims(load_image_into_numpy_array(image_path), axis = 0)\n",
    "    \n",
    "@tf.function\n",
    "def detect(input_tensor):\n",
    "    '''Run detection on an input image.\n",
    "    Args:\n",
    "        input_tensor: A list with [1, height, width, 3] shape of type tf.float32.\n",
    "    Returns:\n",
    "        A dictionary containing 3 tensors (detection_boxes, detection_classes, detection_scores)\n",
    "    '''\n",
    "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
    "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
    "    return detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "\n",
    "for i in range(len(test_images_np)):\n",
    "    input_tensor = tf.convert_to_tensor(test_images_np[i], dtype = tf.float32)\n",
    "    detections = detect(input_tensor)\n",
    "    \n",
    "    plot_detections(\n",
    "        test_images_np[i][0],\n",
    "        detections['detection_boxes'][0].numpy(),\n",
    "        detections['detection_classes'][0].numpy()/astype(np.uint32) + label_id_offset, \n",
    "        detections['detection_scores'][0].numpy(),\n",
    "        category_index, figsize = (15, 20),\n",
    "        image_name = \"../inputs/gif_frame_\" + ('%02d' % i) + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3ec07-5155-452a-a52d-9eebd8c96c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1.jpg  out3.jpg  out5.jpg  out7.jpg\tout9.jpg\n",
      "out2.jpg  out4.jpg  out6.jpg  out8.jpg\n"
     ]
    }
   ],
   "source": [
    "!ls ../inputs/rubber_duck/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4e74882-57ea-467b-902c-907ff5528287",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Zero images were written.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     image \u001b[38;5;241m=\u001b[39m imageio\u001b[38;5;241m.\u001b[39mimread(filename)\n\u001b[1;32m     11\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mimageio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmimsave\u001b[49m\u001b[43m(\u001b[49m\u001b[43manim_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGIF-FI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m display(IPyImage(\u001b[38;5;28mopen\u001b[39m(anim_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread()))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/imageio/core/functions.py:424\u001b[0m, in \u001b[0;36mmimwrite\u001b[0;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# Check that something was written. Check after writing, because ims might\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# be a generator. The damage is done, but we want to error when it happens.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m written:\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZero images were written.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# Return a result if there is any\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m writer\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mget_result()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Zero images were written."
     ]
    }
   ],
   "source": [
    "imageio.plugins.freeimage.download()\n",
    "\n",
    "anim_file = '../inputs/duckies_test.gif'\n",
    "filenames = glob.glob('../inputs/gif_frame_*.jpg')\n",
    "filenames = sorted(filenames)\n",
    "last = -1\n",
    "images = []\n",
    "\n",
    "for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    images.append(image)\n",
    "    \n",
    "imageio.mimsave(anim_file, images, 'GIF-FI', fps = 5)\n",
    "\n",
    "display(IPyImage(open(anim_file, 'rb').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb189f58-2297-4f5f-a452-12166274cf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
